"""ML-TrainA code to solve an input-output problem with the Feed forward Neural Network"""import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport scipy.ioimport pandasimport sklearnfrom tensorflow import kerasfrom keras import modelsfrom keras import layersfrom keras.layers import Inputfrom keras.layers import Densefrom keras.models import Modelfrom sklearn.model_selection import train_test_split#from IPython.display import Imageif __name__ == "__main__":    # load the generated data and obtain the values for x, t:    data = scipy.io.loadmat('Data/database.mat')    # Import dataset    x = data["datafra"]                             # 188232 x 9 doubleï¼š fraction of volume    t = data["datacur"]                             # 188232 x 1 double:  generated curvature    # Process the raw data    # split the dataset to 70% Training set, 15% test set and 15% validation set    X_train, X_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=321)    X_test, X_val, t_test, t_val = train_test_split(X_test, t_test, test_size=0.5, random_state=321)    disp_Datashape = [["Input data", str(x.shape)], ["Output data", str(t.shape)],                      ["X_Train", str(X_train.shape)], ["t_Train", str(t_train.shape)],                      ["X_Test", str(X_test.shape)], ["t_test", str(t_test.shape)],                      ["X_Val", str(X_val.shape)], ["t_val", str(t_val.shape)]]    print("{:<15} {:<15}".format('Name', 'Datashape'))    for i in disp_Datashape:        name, datashape = i        print("{:<15} {:<15}".format(name, datashape))    # build the training model with sequential model    inputs = Input(shape=(9,), name='input_layer')    # preprocess = layers.Normalization(mean=0., variance=1.)(inputs)    # Use like the authors only one hidden layer    # Use the relu activation function instead of tanh    hidden1 = Dense(100,                     kernel_initializer=tf.keras.initializers.GlorotUniform(),                     activation='relu',                     name='hidden_layer_1')(inputs)    # dropout1 = layers.Dropout(rate=0.15, input_shape=(100,), seed=321)(hidden1)    # hidden2 = Dense(100,     #                 kernel_initializer=tf.keras.initializers.GlorotUniform(),     #                 activation='tanh',     #                 name='hidden_layer_2')(hidden1)    # dropout2 = layers.Dropout(rate=0.15, input_shape=(100,), seed=321)(hidden2)    # Use kernel initializer    outputs = Dense(1,                     kernel_initializer=tf.keras.initializers.GlorotUniform(),                     name='output_layer')(hidden1)    feed_forward_NN = Model(inputs=inputs,                            outputs=outputs,                            name='feed_forward_NN')    # print the NN architecture and the NN topology    feed_forward_NN.summary()    # keras.utils.plot_model(feed_forward_NN, to_file='architectureNN.png')    # Image('architectureNN.png')    # Configurate the training method    # Set learning rate to 0.001    # feed_forward_NN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-8),    #                         loss=tf.losses.MeanSquaredError(),    #                         metrics=['mae'],    #                         run_eagerly=False)    feed_forward_NN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),                            loss=tf.keras.losses.MeanSquaredError(),                            metrics=['mae'],                            run_eagerly=False)    print("Fit model on training data")    # history = feed_forward_NN.fit(X_train,    #                               t_train,    #                               epochs=1000,    #                               verbose=2,    #                               batch_size=128,    #                               shuffle=False,    #                               validation_data=(X_val, t_val))    train_batch_size = X_train.shape[0]    history = feed_forward_NN.fit(X_train,                                  t_train,                                  epochs=1000,                                  verbose=2,                                  batch_size=64,                                  shuffle=False,                                  validation_data=(X_val, t_val))    # Evaluate the model with test dataset    print("Evaluate on test data")    # Evaluate test data instead of validation data    #results = feed_forward_NN.evaluate(X_val, t_val, batch_size=32)    results = feed_forward_NN.evaluate(X_test, t_test, batch_size=32)    mae = history.history['mae']    val_mae = history.history['val_mae']    epochs = range(1, len(mae)+1)    print("test loss, test mae:", results)    # Prediction with 1000 sample points in test dataset    num_pred_points = 100000    X_pred = X_train[:num_pred_points]    t_pred_true = t_train[:num_pred_points]    print("Generate predictions")    t_pred = feed_forward_NN.predict(X_pred)    print("prediction shape:", t_pred.shape)    # Plotting the result    # plotting the convergence history    plt.figure('convergence history2')    plt.title('convergence history2')    #plt.xlabel('Iterations')    plt.xlabel('Epochs')    plt.ylabel('log(loss)')    plt.legend('loss')    plt.semilogy(epochs, history.history['loss'], c='blue', label='training loss')    plt.semilogy(epochs, history.history['val_loss'], c='orange', label='validation loss')    plt.legend()    # Save figure    plt.savefig('convergence_history.pdf', format='pdf', bbox_inches='tight')    plt.show()    # Plotting the model accuracy    plt.figure('training accuracy')    plt.title('training accuracy')    plt.plot(epochs, mae, color='b', label='Training mae')    plt.plot(epochs, val_mae, color='orange', label='Validation mae')    plt.xlabel('training epochs')    # The abbreviation for mean squared error is mse and not mae. mae is the abbreviation for mean absolute error.    plt.ylabel('MeanSquaredError')    plt.legend()    # Save figure    plt.savefig('training_accuracy.pdf', format='pdf', bbox_inches='tight')    plt.show()    # # Plotting the prediction    # Plot prediction against target    # plt.figure('prediction output')    # plt.title('prediction output')    # plt.xlabel('Input Testdata')    # plt.ylabel('predicted curvature')    # plt.legend('predicted data')    # plt.scatter(np.arange(100), t_test[:100], marker='o', c='orange', label='target value from test set')    # plt.scatter(np.arange(100), predictions[:100], marker='o', c='blue', label='predict output from NN')    # plt.legend()    #     # Save figure    # plt.savefig('prediction.pdf', format='pdf', bbox_inches='tight')    # plt.show()    plt.figure('prediction vs target')    plt.title('prediction vs target')    plt.xlabel('target')    plt.ylabel('prediction')    # Set x and y axes to the same scale    plt.xlim(-0.25, 0.25)    plt.ylim(-0.25, 0.25)    plt.scatter(t_pred_true, t_pred, marker='o', c='blue', label='data')    plt.legend()    # Save figure    plt.savefig('prediction_vs_target.pdf', format='pdf', bbox_inches='tight')    plt.show()